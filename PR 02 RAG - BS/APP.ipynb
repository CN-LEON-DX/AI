{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2597d24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import tempfile\n",
    "import os \n",
    "import torch\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain import hub\n",
    "\n",
    "from langchain_core.output_parsers import  StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c41d5c63",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33;43m\"\u001b[39;49m\u001b[33;43mrag_chain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mst\u001b[49m\u001b[43m.\u001b[49m\u001b[43msession_state\u001b[49m:\n\u001b[32m      2\u001b[39m     st.session_state.rag_chain = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodels_load\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m st.session_state:\n",
      "\u001b[31mTypeError\u001b[39m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "if \"rag_chain\" not in st.session_state:\n",
    "    st.session_state.rag_chain = None\n",
    "if \"models_load\" not in st.session_state:\n",
    "    st.session_state.models_loaded = False\n",
    "if \"embeddings\" not in st.session_state:\n",
    "    st.session_state.embeddings = None\n",
    "if \"llm\" not in st.session_state:\n",
    "    st.session_state.llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1396e5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model\n",
    "@st.cache_resource\n",
    "def load_embeddings():\n",
    "    return HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16b0e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache_resource\n",
    "def load_llm():\n",
    "    MODEL_NAME = \"lmsys/vicuna-7b-v1.5\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype = torch.bfloat16, \n",
    "        low_cpu_mem_usage = True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model_pipline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=512,\n",
    "        pad_token_id = tokenizer.eos_token_id,\n",
    "        device_map = 'auto'\n",
    "    )\n",
    "\n",
    "    return HuggingFacePipeline(pipeline=model_pipline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b662e9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# func for pdf\n",
    "\n",
    "def process_pdf(uploaded_file):\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
    "        tmp_file.write(uploaded_file.getvalue())\n",
    "        tmp_file_path = tmp_file.name\n",
    "\n",
    "    # save file temp\n",
    "\n",
    "    # read file \n",
    "    loader = PyPDFLoader(tmp_file_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # semantic chunking\n",
    "    semantic_splitter = SemanticChunker(\n",
    "        embeddings=st.session_state.embeddings,\n",
    "        buffer_size=  1, \n",
    "        breakpoint_threshold_type=\"percentile\",\n",
    "        breakpoint_threshold_amount=95,\n",
    "        min_size_chunk = 500,\n",
    "        add_start_index=True\n",
    "    )\n",
    "\n",
    "\n",
    "    docs = semantic_splitter.split_documents(documents)\n",
    "    vector_db = Chroma.from_documents(documents=docs, embedding=st.session_state.embeddings)\n",
    "\n",
    "    retriever = vector_db.as_retriever() # get db in this doc for searching !\n",
    "\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\") # a sample prompt like: Base on context: {context}\n",
    "    # and answer this question: {question}\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    rag_chain = (\n",
    "        {\"context\":retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | st.session_state.llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    os.unlink(tmp_file_path) # clean temp file after run\n",
    "\n",
    "    return rag_chain, len(docs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f92d4291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# buid UI\n",
    "\n",
    "st.set_page_config(page_title=\"PDF RAG Assistant\", layout=\"wide\")\n",
    "st.title(\"PDF RAG Assistant\")\n",
    "\n",
    "st.markdown(\"\"\"\n",
    "**AI Application helps you can chat with PDF file**\n",
    "**How to use ? ** \n",
    "1. **Upload file PDF** Upload your file then press Process\n",
    "2. **Ask any question** Ask\n",
    "--- \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc9c2928",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'models_loaded'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#load model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mst\u001b[49m\u001b[43m.\u001b[49m\u001b[43msession_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels_loaded\u001b[49m:\n\u001b[32m      4\u001b[39m     st.info(\u001b[33m\"\u001b[39m\u001b[33mLoading models ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     st.session_state.embeddings = load_embeddings()\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'models_loaded'"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "\n",
    "if not st.session_state.models_loaded:\n",
    "    st.info(\"Loading models ...\")\n",
    "    st.session_state.embeddings = load_embeddings()\n",
    "\n",
    "    st.session_state.llm = load_llm()\n",
    "    st.session_state.model_loaded = True\n",
    "    st.success(\"Models ready \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3349e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload file and process file pdf\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Upload file PDF\", type ='pdf')\n",
    "if uploaded_file and st.button(\"Process file PDF\"):\n",
    "    with st.spinner(\"Processing\"):\n",
    "        st.session_state.rag_chain, num_chunks = process_pdf(uploaded_file)\n",
    "\n",
    "        st.success(f'Complete ! {num_chunks} chunks')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6885f8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'rag_chain'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# UI QA\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mst\u001b[49m\u001b[43m.\u001b[49m\u001b[43msession_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrag_chain\u001b[49m:\n\u001b[32m      3\u001b[39m     question = st.text_input(\u001b[33m\"\u001b[39m\u001b[33mQuestion:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m question:\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'rag_chain'"
     ]
    }
   ],
   "source": [
    "# UI QA\n",
    "if st.session_state.rag_chain:\n",
    "    question = st.text_input(\"Question:\")\n",
    "\n",
    "    if question:\n",
    "        with st.spinner(\"Replying\"):\n",
    "            output = st.session_state.rag_chain.invoke(question)\n",
    "            answer = output.split(\"Answer:\")[1].strip() if \"Anwser:\" in output else output.strip()\n",
    "        \n",
    "            st.write(\"**Answer: **\")\n",
    "            st.write(answer)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58976c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
