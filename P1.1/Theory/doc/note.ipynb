{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca58be15",
   "metadata": {},
   "source": [
    "Sigmoid Function == logistic function, \n",
    "\n",
    "Sigmoid chuyển đổi mọi giá trị đầu vào thành một giá trị đầu\n",
    "ra nằm giữa 0 và 1, với một sự chuyển đổi mượt mà tại giá trị 0.\n",
    "\n",
    "Ứng Dụng: \n",
    "+ Sigmoid Function thường được sử dụng trong các bài toán phân loại nhị phân, nơi\n",
    "mà mục tiêu là phân loại đầu vào thành một trong hai lớp.\n",
    "\n",
    "Ưu Điểm:\n",
    "• Dễ hiểu và triển khai: Do tính chất đơn giản và phổ biến, Sigmoid rất được triển khai\n",
    "trong nhiều loại mạng neuron.\n",
    "• Đầu ra nằm trong khoảng (0,1): Giá trị đầu ra luôn nằm trong khoảng từ 0 đến 1, giúp\n",
    "dễ dàng diễn giải như là xác suất.\n",
    "\n",
    "Nhược điểm:\n",
    "• Vanishing gradient problem: Khi đầu vào có giá trị lớn hoặc nhỏ, đạo hàm của Sigmoid\n",
    "tiệm cận đến 0, dẫn đến vấn đề vanishing gradient, làm chậm quá trình học của mạng.\n",
    "\n",
    "• Tâm đối xứng không nằm tại 0: Tâm đối xứng không nằm tại điểm 0, điều này có thể\n",
    "gây ra vấn đề trong việc điều chỉnh trọng số trong quá trình học.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a915d981",
   "metadata": {},
   "source": [
    "Tanh Function: \n",
    "Tanh Function, viết tắt từ hyperbolic tangent function, là một hàm toán\n",
    "học phổ biến trong machine learning và neural networks. Tanh giúp chuyển đổi giá trị đầu vào\n",
    "thành khoảng giá trị nằm trong (-1, 1).\n",
    "Ứng Dụng: Tanh Function thường được sử dụng như một hàm kích hoạt trong neural networks\n",
    "và có nhiều ứng dụng quan trọng. Ví dụ được áp dụng trong các lớp ẩn của neural networks để\n",
    "giúp mô hình học các biểu diễn phức tạp.\n",
    "Ưu Điểm:\n",
    "• Đối xứng tại 0 khi x nhỏ: Giá trị đầu ra của Tanh nằm gần trung bình 0, giúp ổn định\n",
    "quá trình học.\n",
    "• Giảm vanishing gradient: Hỗ trợ giảm nguy cơ vanishing gradient, tăng cường khả năng\n",
    "học của mô hình.\n",
    "Nhược điểm:\n",
    "• Bão Hòa (Saturation): Có khả năng bị saturation ở giá trị cực đại hoặc cực tiểu.\n",
    "• Giá trị không đối xứng tại 0 cho dữ liệu đầu vào không được chuẩn hóa: Nếu dữ\n",
    "liệu đầu vào không được chuẩn hóa, giá trị trung bình của đầu ra có thể không gần 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc5fcb",
   "metadata": {},
   "source": [
    "Softplus Function: Softplus Function là một hàm toán học thường được sử dụng trong\n",
    "machine learning và neural networks. Softplus giúp chuyển đổi giá trị đầu vào thành một khoảng\n",
    "không giới hạn trong đoạn (0, +∞).\n",
    "Ứng Dụng: Softplus Function thường được sử dụng trong các tình huống đòi hỏi một hàm kích\n",
    "hoạt có dạng tăng nhanh và không giữ giá trị đầu ra trong một khoảng cụ thể. Softplus thường\n",
    "được sử dụng làm hàm kích hoạt trong các lớp ẩn của neural networks, đặc biệt là trong các mô\n",
    "hình mà yêu cầu tính không giới hạn của đầu ra.\n",
    "Ưu Điểm:\n",
    "• Không giới hạn đầu ra: Cho phép giá trị đầu ra không giới hạn và tăng lên nhanh chóng\n",
    "với giá trị đầu vào dương.\n",
    "• Khả năng xấp xỉ hàm ReLU: Trong một số trường hợp, Softplus có khả năng xấp xỉ hàm\n",
    "Rectified Linear Unit (ReLU).\n",
    "Nhược điểm: Có khả năng bị bão hòa (saturation) và dễ mất đi ở giá trị âm đối với giá trị đầu\n",
    "vào lớn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca16124b",
   "metadata": {},
   "source": [
    "ReLU Function: ReLU, viết tắt của \"Rectified Linear Unit\", là một hàm kích hoạt rất phổ\n",
    "biến trong neural networks. Được đánh giá cao vì tính đơn giản nhưng hiệu quả, ReLU được sử\n",
    "dụng rộng rãi để giúp neural networks học được những đặc trưng phức tạp mà không gặp phải\n",
    "vấn đề biến mất gradient (gradient vanishing). ReLU hoạt động dựa trên nguyên tắc rất đơn giản:\n",
    "nếu đầu vào là số âm, hàm sẽ trả về giá trị 0, còn nếu đầu vào là số dương, hàm sẽ trả về chính\n",
    "giá trị đó.\n",
    "Ứng dụng: ReLU phổ biến trong các ứng dụng như nhận dạng hình ảnh và xử lý ngôn ngữ tự\n",
    "nhiên, nơi ReLU giúp cải thiện tốc độ học và giảm thiểu vấn đề biến mất gradient. ReLU cũng\n",
    "rất quan trọng trong học tăng cường và các tác vụ phân loại, cung cấp một phương pháp hiệu\n",
    "quả để xử lý thông tin phi tuyến tính.\n",
    "Ưu điểm:\n",
    "• Tính toán đơn giản: Do cấu trúc đơn giản, ReLU nhanh và hiệu quả hơn trong việc tính\n",
    "toán so với các hàm kích hoạt phi tuyến tính khác.\n",
    "• Giảm mất mát gradient: ReLU giúp giảm thiểu vấn đề biến mất gradient, một điểm mạnh\n",
    "quan trọng trong quá trình huấn luyện neural networks.\n",
    "Nhược điểm:\n",
    "• Vấn đề dying ReLU: Đôi khi neuron có thể chỉ trả về giá trị 0 cho tất cả các đầu vào, dẫn\n",
    "đến hiện tượng \"dying ReLU\", khi đó neuron trở nên không hoạt động.\n",
    "• Không đối xứng tại zero: Do ReLU không phải là hàm có giá trị trung bình bằng 0 nên\n",
    "có thể gây ra vấn đề trong quá trình tối ưu hóa mạng."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df410055",
   "metadata": {},
   "source": [
    "LeakyReLU Function: LeakyReLU, một biến thể của hàm ReLU, cũng là một hàm kích\n",
    "hoạt phi tuyến tính được sử dụng trong neural networks. Điểm đặc biệt của LeakyReLU là không\n",
    "trả về giá trị 0 cho các giá trị đầu vào âm, mà thay vào đó là một giá trị nhỏ, thường được đặt là\n",
    "0.01x.\n",
    "Ứng dụng: LeakyReLU được sử dụng rộng rãi trong xử lý ảnh và nhận dạng hình ảnh, cung cấp\n",
    "khả năng học các đặc trưng phức tạp mà không gặp phải vấn đề \"dying ReLU\". Trong lĩnh vực\n",
    "học tăng cường và xử lý ngôn ngữ tự nhiên, LeakyReLU giúp cải thiện đáng kể hiệu suất của các\n",
    "mô hình bằng cách giảm thiểu biến mất gradient, một vấn đề thường gặp trong quá trình huấn\n",
    "luyện mạng nơ-ron sâu.\n",
    "Ưu điểm:\n",
    "• Giảm thiểu vấn đề dying ReLU: Bằng cách cho phép trả về một giá trị nhỏ thay vì 0 cho\n",
    "các đầu vào âm, LeakyReLU giúp giảm thiểu vấn đề các neuron trở nên không hoạt động.\n",
    "• Phù hợp cho deep neural network: Tương tự như ReLU, LeakyReLU cũng giúp giải\n",
    "quyết vấn đề biến mất gradient, làm cho việc huấn luyện deep neural network trở nên hiệu\n",
    "quả hơn.\n",
    "Nhược điểm:\n",
    "• Dùng giá trị cứng 0.01: Việc dùng giá trị 0.01 làm cho activation này không có tính tổng\n",
    "quát.\n",
    "• Không đối xứng tại zero: Tương tự như ReLU, LeakyReLU cũng không phải là hàm có\n",
    "giá trị trung bình bằng 0, có thể gây ra vấn đề trong quá trình tối ưu hóa mạng."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc2d9df",
   "metadata": {},
   "source": [
    "ELU Function: \n",
    "ELU, viết tắt của Exponential Linear Unit, là một loại activation function\n",
    "được sử dụng trong neural network. Được đề xuất bởi Djork-Arné Clevert và các cộng sự vào năm\n",
    "2015, ELU nhằm mục đích giải quyết một số vấn đề của các activation function trước đây như\n",
    "ReLU. ELU có công thức (6) giúp ELU giảm thiểu vấn đề vanishing gradient ở các giá trị âm,\n",
    "đồng thời vẫn duy trì tính phi tuyến cần thiết cho quá trình học sâu.\n",
    "Ứng Dụng: ELU chủ yếu được sử dụng trong các mạng neuron sâu, đặc biệt là những nơi cần\n",
    "giải quyết vấn đề vanishing gradient. ELU thường được áp dụng trong các mô hình học sâu phức\n",
    "tạp như convolutional neural networks (CNNs) và recurrent neural networks (RNNs) để cải thiện\n",
    "tốc độ học và hiệu suất của mô hình.\n",
    "Ưu Điểm:\n",
    "• Hiệu suất cao: Trong một số trường hợp, ELU cho thấy hiệu suất tốt hơn so với các\n",
    "activation function khác như ReLU và Leaky ReLU, đặc biệt trong các mạng sâu.\n",
    "\n",
    "• Có đầu ra âm: Việc này giúp duy trì một phân phối đầu ra cân bằng hơn, có thể cải thiện\n",
    "khả năng học của mô hình.\n",
    "Nhược Điểm:\n",
    "• Tính toán phức tạp hơn: Do cấu trúc phức tạp của công thức, ELU đòi hỏi nhiều chi phí\n",
    "tính toán hơn so với ReLU.\n",
    "• Lựa chọn α: Việc lựa chọn giá trị của α có thể ảnh hưởng đáng kể đến hiệu suất của mô\n",
    "hình và nó không có một quy tắc cụ thể nào, đòi hỏi phải thử nghiệm để tìm ra giá trị phù\n",
    "hợp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0704f173",
   "metadata": {},
   "source": [
    "PReLU Function: PReLU, viết tắt của Parametric Rectified Linear Unit (đơn vị tuyến tính điều chỉnh tham số ??), là một biến thể\n",
    "của ReLU trong neural network. Được giới thiệu trong một nghiên cứu năm 2015, PReLU làm\n",
    "cho hệ số độ dốc của phần âm của ReLU có thể học được trong quá trình huấn luyện, thay vì là\n",
    "một hằng số cố định như trong Leaky ReLU. Ở đây, α là một tham số học được, không phải là\n",
    "một giá trị cố định. Điều này giúp PReLU linh hoạt hơn so với ReLU và Leaky ReLU\n",
    "\n",
    "Ứng Dụng: PReLU thường được sử dụng trong các mô hình học sâu, đặc biệt là trong các mạng\n",
    "neuron sâu như CNNs và RNNs. PReLU giúp cải thiện hiệu suất của mô hình trong các tác vụ\n",
    "phức tạp như nhận dạng hình ảnh, xử lý ngôn ngữ tự nhiên, và học tăng cường.\n",
    "\n",
    "Ưu Điểm:\n",
    "• Tính linh hoạt cao: Do α là tham số học được, PReLU có khả năng tự điều chỉnh dựa\n",
    "trên dữ liệu, giúp cải thiện hiệu suất.\n",
    "• Hiệu suất tốt trong mô hình sâu: Trong nhiều trường hợp, PReLU đã chứng minh có\n",
    "hiệu suất tốt hơn ReLU, đặc biệt trong các mô hình sâu và phức tạp.\n",
    "\n",
    "\n",
    "Nhược Điểm:\n",
    "• Nguy cơ overfitting: Do tính chất tham số hóa, nếu không được điều chỉnh cẩn thận,\n",
    "PReLU có thể dẫn đến overfitting, đặc biệt khi sử dụng trong các mạng có ít dữ liệu.\n",
    "• Tăng chi phí tính toán: Việc học các tham số α thêm vào chi phí tính toán và thời gian\n",
    "huấn luyện của mô hình.\n",
    "• Cần tinh chỉnh thêm: Việc tìm ra giá trị tối ưu của α đòi hỏi quá trình thử nghiệm và\n",
    "tinh chỉnh, có thể làm phức tạp quá trình huấn luyện."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d75a6c1",
   "metadata": {},
   "source": [
    "Swish Function: \n",
    "\n",
    "Swish Function là một loại activation function được phát triển bởi các\n",
    "nhà nghiên cứu tại Google. Được giới thiệu lần đầu trong một bài báo năm 2017, Swish đã nhanh\n",
    "chóng thu hút sự chú ý trong cộng đồng học máy vì tính hiệu quả và độc đáo của nó. Công thức\n",
    "của Swish (8), trong đó x là đầu vào, và β là một tham số có thể học nhưng được dùng với mặc\n",
    "định là 1.0. Swish kết hợp lợi ích của việc sử dụng thông tin đầu vào (như ReLU) và tránh được\n",
    "dying ReLU.\n",
    "\n",
    "Ứng Dụng: Swish thường được sử dụng trong các mô hình học sâu, như CNNs và RNNs. Swish\n",
    "đặc biệt hữu ích trong các tác vụ liên quan đến học sâu và học tăng cường, nơi cần một activation\n",
    "function hiệu quả và linh hoạt.\n",
    "\n",
    "Ưu Điểm:\n",
    "• Hiệu suất cao trong mô hình sâu: Swish thường cho thấy hiệu suất tốt trong các mạng\n",
    "neuron sâu, đôi khi vượt trội so với các activation function truyền thống như ReLU.\n",
    "• Liên Tục và Khả Vi: Điều này giúp trong quá trình cập nhật trọng số qua backpropagation\n",
    "diễn ra mượt mà hơn so với các hàm như ReLU.\n",
    "\n",
    "Nhược Điểm:\n",
    "• Tính toán phức tạp hơn: So với các activation function đơn giản như ReLU, Swish có\n",
    "công thức phức tạp hơn, dẫn đến tăng chi phí tính toán.\n",
    "• Ít Thông Dụng: Do là hàm mới hơn so với các hàm kích hoạt truyền thống như ReLU và\n",
    "Sigmoid, Swish chưa được sử dụng rộng rãi và nghiên cứu kỹ lưỡng."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004bf1dd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
